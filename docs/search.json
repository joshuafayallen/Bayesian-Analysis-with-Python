[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Analysis with Python: Notebook",
    "section": "",
    "text": "Preface\nThis is my notebook for Bayesian Analysis with Python. I deviate slightly from the book because I am more of a polars person than a pandas person. If you notice anything that could be improved or any typos please feel free to raise an issue. I am also playing around with marimo and quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html",
    "href": "thinking-probabilistically.html",
    "title": "1¬† Thinking Probabilistically",
    "section": "",
    "text": "1.1 A Probability Primer for Bayesian Practitioners",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#a-probability-primer-for-bayesian-practitioners",
    "href": "thinking-probabilistically.html#a-probability-primer-for-bayesian-practitioners",
    "title": "1¬† Thinking Probabilistically",
    "section": "",
    "text": "1.1.1 Sample space and events\nLets say we go out and ask 3 random respondents a series of three questions and record their answers. There are lots of ways we could represent. In Table¬†1.1 I remix the example in the book a bit but the core idea is the same. The sample space is the set of all possible outcomes.\n\n\n\n\nTable¬†1.1\n\n\n\n\n\n\n\n\n\nQuestion\nIndividual\nResponse\n\n\n\n\n1\nPerson 1\nyes\n\n\n2\nPerson 1\nyes\n\n\n3\nPerson 1\nyes\n\n\n1\nPerson 2\nno\n\n\n2\nPerson 2\nno\n\n\n3\nPerson 2\nno\n\n\n\n\n\n\n\n\n\n\n\nIn our case, we are just talking about the order of each person‚Äôs responses. An event is a subset of the feature space, so we can think of it as the order in which people respond to the questions. So there are eight possible combinations of answer orders. For brevity, I only include two of those events. If we wanted to know how likely it is that we observe an event, we can express it as a simple proportion.\n\\[\nP(Event) = \\frac{\\text{event we are interested in}}{\\text{total number of events}}\n\\]\nSo if we are interested in the all yes event, it is just 0.125. If we make the assumption that the events are independent of each other then the probability of observing an event where the respondent answers all yes and another respondent answers all no than we can represent that as:\n\\[\nP(\\text{All Yes and No}) = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = 0.25\n\\]\nFast forward a bit the three axioms of probability are:\n\nThe probability of an event is an even is a non-negative real number\nThe probability of P(sample space) = 1\nIf ùê¥1, ùê¥2,‚Ä¶ are disjoint events, meaning they cannot occur simultaneously then ùëÉ(ùê¥1, ùê¥2, ‚Ä¶) = ùëÉ(ùê¥1) + ùëÉ(ùê¥2) + ‚Ä¶\n\n\n\n1.1.2 Random Variables\nA random variable is effectively a function that maps the sample space the real numbers. So we can think of that as. Doing something akin to:\n\n\n\n\n\n\n\n\nQuestion\nIndividual\nResponse\nMapping Examp\n\n\n\n\n1\nPerson 1\nyes\n1\n\n\n2\nPerson 1\nyes\n1\n\n\n3\nPerson 1\nyes\n1\n\n\n1\nPerson 2\nno\n0\n\n\n2\nPerson 2\nno\n0\n\n\n3\nPerson 2\nno\n0\n\n\n\n\n\n\n\n\nThe author uses the example of a set of Python code. So if the take example from pl.when(thing_happens).then(do this).otherwise(do that) this is an abstraction over a set of Rust code that takes our inputs and then maps them to our desired output. We could theoretically write out all this Rust code every single time. But we package it in a nice set of functions to make the end user‚Äôs code cleaner and reusable we make some functions to do this.\nRandom variables play this similar role in stats where we have some inputs like responses to a survey or temperature that get mapped onto some process. There is no randomness when moving from ‚Äúyes‚Äù to ‚Äúone‚Äù, but there is some randomness when we go to the survey the third and fourth person to get their event.\n\n\n1.1.3 Discrete random variables and their distributions\nWe generally don‚Äôt care about the kidns of problems that get presented in these refreshers. The probability of 3 inidividuals answering all yes or the probability of getting a one on the dice. In general we are more intersted in getting a list of probabilities for all possible answers. We can then get the possibilities of any sort of event we would care about. As a simple heuristic the list of the probabilities are effectively just this list of probabilities. So if we wanted to get the proportion of heads of 1000 flipped coins we could do this.\n\n\n\n\n\n\n\n(array([ 13.,  25., 174., 787.,   0.,   0.,   0.,   0.,   0.,   1.]),\n array([0.31818182, 0.38636364, 0.45454545, 0.52272727, 0.59090909,\n        0.65909091, 0.72727273, 0.79545455, 0.86363636, 0.93181818,\n        1.        ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure¬†1.1\n\n\n\n\nIn this case the probability distribution of a coin flip is a Bernouli distribution. When we have multiple Bernouli trials we can stack them and make a binomial. One thing about these functions is that they have, for lack of a better term, shape parameters. For a normal those are our mean and standard deviation for something like the beta binomial in rbinom() it is rbinom(shape1 = 2, shape2 = 9) which changes where the most likely outcomes are.\n\n\nCode\nnp.random.seed(1994)\n\nfrom cycler import cycler\ndefault_cycler = cycler(color=[\"#000000\", \"#6a6a6a\", \"#bebebe\", \"#2a2eec\"])\n\nplt.rc('axes', prop_cycle=default_cycler)\nplt.rc('figure', dpi=600)\n\nn = 5\n\nx = np.arange(0,5)\nfig, axes = plt.subplots(2,2, figsize = (10,6), sharey= True, sharex = True)\naxes = np.ravel(axes)\n\n\nfor ax, (alpha, bt) in zip(axes, ((1,1), (6,4),(4,6), (20,20))):\n    dist_pmf = binom(n, x) * (beta(x+alpha, n-x+bt) / beta(alpha, bt))\n    ax.vlines(x, 0, dist_pmf, colors = 'C0', lw=4)\n    ax.set_title(f\"Œ±={alpha}, Œ≤={bt}\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(x + 1)\n\nfig.text(0.52, -0.04, \"x\", fontsize=18)\nfig.text(-0.04, 0.4, \"P(X=x)\", fontsize=18, rotation=\"vertical\")\n\n\nText(-0.04, 0.4, 'P(X=x)')\n\n\n\n\n\n\n\n\n\nWhile this is helpful for us as we are toying around with setting priors or as pedagogy. The thing that is doing the heavy lifting of mapping things to the like P(thing) = 0.6 is the probability mass function. This works for discrete data because you can‚Äôt have 1.5 heads. I eschew including it but one really helpful thing to help us wrap our heads around what changing the shape parameters of our distribution are is using the interactive plots.\n\nbb = pz.BetaBinomial(alpha = 10, beta = 10, n = 6)\n\nbb.plot_interactive(figsize = (10,6), pointinterval=False, xy_lim =(20,20))\n\n\n\n\n\n\n1.1.4 Continous random variables and their distributions\nContinous values are a bit different. They, theoretically, can go go from \\(-\\infty\\) to \\(\\infty\\) so the mapping has to work a little bit differently. Instead of a probalility mass function where we are moving from heads to 1 we have to effectively create bins and we count how many values fall in each bin. If we take a simple example of going from -1 to 1 we can get a huge number of points. So the actual probablity of getting exactly one in this context is 0.01 making it somewhat useless to do probablility like this. Hence why we take this density approach.\n\n\n1.1.5 Cumulative distribution function\nFinally we have the CDF to characterize distributions. The CDF of a random variable is kind of just the idea that we can take the mappings of a PMF or a PDF and then just map them onto a fairly simple question. What is the probability we get a point as big or less than the point we drew from that distribution.\n\n\n1.1.6 Conditional Probablities\nFrom the highest level of abstraction all our models are really just fancy conditional probability machines. Given an outcome variable how does it change if we observe these 1 to a million things. To keep it more manageable than lets just consider the bivariate case. If we are trying to understand how something about how good an NFL team is we might want to look at their EPA and condition on adjusted games lost. The more games that a team‚Äôs best players lose the worse their EPA is going to be. By knowing something about EPA we know a little bit something about their\nWe would anticipate that the higher a team‚Äôs EPA is the less adjusted games lost they have. If we plot the marginal distributions we can see that. Quickly to compute the marginal distribution of AGL we are going to take the p(AGL, EPA) and then we average over EPA. In Figure¬†1.2 we can see that a bit more clearly. The dashed lines are the conditional probabilities when we sample from the p(AGL, EPA).\n\n\nCode\n# take steps of 0.1 from -4 to 4 effectively just seq(-4, 4, by = 0.1)\nx, y = np.mgrid[-4:4:.01, -4:4:.01]\npos = np.empty(x.shape + (2,))\npos[:, :, 0] = x; pos[:, :, 1] = y\n\n## fill the empty 3d array \npos[:, :, 0] = x; pos[:, :, 1] = y\n\n## initialize a multivariate normal \n\nrv = pz.MvNormal([0, 0], [[1, 0.8], \n                          [0.8, 1]])\n\n\nx_value = pos[:, :, 0][:,0]\nx_density = rv.pdf(pos)\n\nleft, width = 0.1, 0.65\nbottom, height = 0.1, 0.65\nbottom_h = left_h = left + width + 0.02\n\nrect_scatter = [left, bottom, width, height]\nrect_histx = [left, bottom_h, width, 0.2]\nrect_histy = [left_h, bottom, 0.2, height]\n\n# this is effectively making small multiples\n# that we then place in the positions defined above \n_, axes_arr = plt.subplots(1, 3, figsize=(8, 8))\n\nax_joint, ax_x, ax_y = axes_arr\nax_joint.set_position(rect_scatter)\nax_x.set_position(rect_histx)\nax_y.set_position(rect_histy)\n\nax_joint.imshow(x_density, cmap='cet_gray_r', origin='lower', extent=[-3, 3, -3, 3])\n\n# these are effectively arbitrary as long as they are within the grid\nax_joint.plot(x_value, x_density[400]*2, 'k:', lw=2)\nax_joint.plot(x_value, x_density[500]*2+1, 'k:', lw=2)\nax_joint.plot(x_value, x_density[300]*2-1, 'k:', lw=2)\n\n\nax_x.fill_between(x_value, x_density.sum(1), color = 'C2')\nax_y.fill_between(x_value, x_density.sum(1), color = 'C2')\n\nfor ax in [ax_joint, ax_x, ax_y]:\n    ax.grid(False)\n    ax.set_facecolor('w')\n    ax.set_xticks([])\n    ax.set_yticks([])\nax_joint.set_xlim(-3, 3)\nax_joint.set_ylim(-3, 3)\nax_x.set_xlim(-3, 3)\nax_y.set_ylim(-3, 3)\nax_x.set_xlim(-3, 3)\nax_joint.set_ylabel('$B$', rotation=0, labelpad=20, fontsize=18)\nax_joint.set_xlabel('$A$', fontsize=18)\n\nax_joint.text(-2.5, 2.5, '$p(EPA, AGL)$', fontsize=18, color='k', weight='medium')\nax_y.text(10, 0, '$p(GPL)$', fontsize=18, color='k', weight='medium')\nax_x.text(-0.2, 15, '$p(EPA)$', fontsize=18, color='k', weight='medium')\nax_joint.text(1, -2, ' ... $p(EPA \\mid AGL)$', fontsize=18, color='k', weight='medium')\n\n\n&lt;&gt;:62: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:62: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/0k/jvdgmwf56zg1dw7n9mm2wzvc0000gp/T/ipykernel_49966/4175977088.py:62: SyntaxWarning: invalid escape sequence '\\m'\n  ax_joint.text(1, -2, ' ... $p(EPA \\mid AGL)$', fontsize=18, color='k', weight='medium')\n\n\n\n\n\n\n\nText(1, -2, ' ... $p(EPA \\\\mid AGL)$')\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure¬†1.2\n\n\n\n\n\n\n1.1.7 Expected Values\nLets say that we have a discrete random variable X. We can compute its expected value as:\n\\[\nE(X) = \\Sigma_{x} xP(X=x)\n\\]\nWhich is just another way of expressing the average. This is one version of an expected value. Instead of doing the usual hand calculating of a mean. The variance is also another form of an expected value. Sometimes we refer to these as the moment of a distribution. In a Gaussian distribution we also have something called kurtosis which tells us how much stuff is in the tails of the distribution. We also has skewness which shifts where the masses are so we could do something like this.\n\n_, ax = plt.subplots(2,2, figsize = (12,6), sharex=True)\n\npz.Normal(0, 1).plot_pdf(ax=ax[0, 0], moments=[\"m\", \"d\", \"s\", \"k\"], legend=\"title\")\npz.BetaScaled(1.1, 1.1, -4, 4).plot_pdf(ax=ax[0, 1], moments=[\"m\", \"d\", \"s\", \"k\"], legend=\"title\")\npz.SkewNormal(0, 1, 3).plot_pdf(ax=ax[1, 0], moments=[\"m\", \"d\", \"s\", \"k\"], legend=\"title\")\npz.StudentT(3, 0, 1).plot_pdf(ax=ax[1, 1], moments=[\"m\", \"d\", \"s\", \"k\"], legend=\"title\", support=(-7, 7))",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html",
    "href": "Programming-Probabilistically.html",
    "title": "2¬† Programming Probabilistically",
    "section": "",
    "text": "2.1 Flipping coins with PyMC\nLast chapter was kind of a primer that is worth going over from time to time. But the nuts and bolts of why you spent the 30ish dolars is to learn PyMc and how to do Bayesian modeling in Python.\nLets repeat the coin flipping problem in PyMC in our case we can set a real value for \\(\\Theta\\) and creatively name it theta real.\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl \nimport polars.selectors as cs\nimport preliz as pz\nimport pymc as pm\n\nnp.random.seed(1994)\n\ntheta_real = 0.35\n\ntrials = 4\n\ndata = pz.Binomial(n = 1, p = theta_real).rvs(trials)\nNo we have a mode lets go and define as\n$$\nWhat is fun about PyMC is that we specify the model in a broadly similar way to Stan. So this model becomes\nwith pm.Model() as our_first_mod:\n    theta = pm.Beta('theta', alpha = 1., beta = 1.)\n    y = pm.Bernoulli('y', p = theta, observed = data)\n    idata = pm.sample(1000, nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.05\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.24\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.04\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.13\n                    3",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#flipping-coins-with-pymc",
    "href": "Programming-Probabilistically.html#flipping-coins-with-pymc",
    "title": "2¬† Programming Probabilistically",
    "section": "",
    "text": "$$\n\\[\\begin{aligned}\n\\Theta \\sim Beta(\\alpha = 1, \\beta = 1) \\\\\nY \\sim Binomial (n = 1, p = \\Theta)\n\n\\end{aligned}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#summarzing-the-posterior",
    "href": "Programming-Probabilistically.html#summarzing-the-posterior",
    "title": "2¬† Programming Probabilistically",
    "section": "2.2 Summarzing the Posterior",
    "text": "2.2 Summarzing the Posterior\nThere are some built in methods to summarize the data and check the health of our chains.\n\naz.plot_trace(idata)\n\narray([[&lt;Axes: title={'center': 'theta_logodds__'}&gt;,\n        &lt;Axes: title={'center': 'theta_logodds__'}&gt;],\n       [&lt;Axes: title={'center': 'theta'}&gt;,\n        &lt;Axes: title={'center': 'theta'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nShe looks pretty good nothing like sticks out all that much. I am probably going to be a little more partial to the ggdist. One of the nice things is that we can extract the data like this.\n\nout = pl.from_pandas(idata.to_dataframe())\n\nout.write_parquet(\"data/first_mod_samps.parquet\")\n\nThen we can then go ahead and load and summarize her like this. This is likely just going to be more helpful if I want to make really specific plots with ggplot than using using arviz. This is nothing personal against arviz I just have more experience in ggplot.\nlibrary(arrow)\nlibrary(ggdist)\nlibrary(tidyverse)\n\nfirst_mod = read_parquet(here::here('data', 'first_mod_samps.parquet')) |&gt;\n    janitor::clean_names()\n\n\nggplot(first_mod, aes(x = posterior_theta)) +\n    stat_halfeye()\n\nIf we want to take a quick peak at some of summary statistics we can do\n\naz.summary(idata, kind = 'stats').round(2)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\ntheta_logodds__\n-2.04\n1.28\n-4.41\n0.25\n\n\ntheta\n0.17\n0.14\n0.00\n0.42\n\n\n\n\n\n\n\nWHich tells us roughly that 97% of the values is are going to be between 0.03 and 0.66 which is a lot more informative in my opinion than point estimates. One of the main benefits of Bayesian statistics is that we generally want to talk about the uncertainty rather than the point estimates.1 Arviz has a built in way to do this\n\naz.plot_posterior(idata, point_estimate='median')\n\narray([&lt;Axes: title={'center': 'theta_logodds__'}&gt;,\n       &lt;Axes: title={'center': 'theta'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nWe can also check the region of practical equivelence. In our coin flipping case this might be some reasonable values like between 45 and 55 for all intents and purposes we have a fair coin we just don‚Äôt have enough data get a mean of like 0.5. We should probably have a better justification for why we chose this region. For a model of vote share a increase of 1-3 percent could be huge!\n\naz.plot_posterior(idata, rope = [0.45, 0.55])\n\narray([&lt;Axes: title={'center': 'theta_logodds__'}&gt;,\n       &lt;Axes: title={'center': 'theta'}&gt;], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#bayes-theorum",
    "href": "thinking-probabilistically.html#bayes-theorum",
    "title": "1¬† Thinking Probabilistically",
    "section": "1.2 Bayes Theorum",
    "text": "1.2 Bayes Theorum\nLets fastforward a bit so we can express Bayes‚Äô rule as\n\\[\n\\overbrace{p(hypothesis | data)}^{Posterior} = \\frac{\\overbrace{p(data | hypothesis)}^{likelihood}\\overbrace{p(hypothesis)}^{prior}}{\\underbrace{P(data)}_{marginal likelihood}}\n\\]\n\nPrior: The prior is a reflection of what we know about the hypothesis prior to ever seeing the data. There are a lot of ways to think of priors which is always something that is a bit difficult to wrangle our head around. In a machine learning framework we could think of a prior as a form of regularization. We don‚Äôt actually think every value from 0-1 are equally likely in a model of EPA. If we draw a QB from a Beta distribution it is unlikely that we are going to draw a QB has an EPA of 0.9.1 From a classical statistical perspective we can really just think of the prior as a more apparent assumption we are making.\nLikelihood: The likelihood is the way we introduce data to our analysis. So in the model it is really just ‚Äúwhat is the probability of the data conditional on the hypothesis‚Äù. So in an OLS the likelihood is just the normal pdf.\nPosterior: The posterior practically is just the product of the likelihood and the prior. So to put this a bit more concretly: it is the probability of the data conditional on the hypothesis multiplied by what believe before seeing the data. This is itself a probability distribution. A conjugate prior of a likelihood is a prior that, when used in combination with a given likelihood, returns a posterior with the same functional form as the prior",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#single-parameter-inference",
    "href": "thinking-probabilistically.html#single-parameter-inference",
    "title": "1¬† Thinking Probabilistically",
    "section": "1.3 Single Parameter inference",
    "text": "1.3 Single Parameter inference\n\n1.3.1 The Coin-flipping problem\nLets say we are not the biggest joy to have at parties. We corner some unlucky party goers and we want to know if the quarter is fair coin. This could be important if we want to model the bias of a coin especially if we are like trying to figure out how to game the coin flip for a game or something. Really this is just a pedagocial example so no need to dwell on it that much.\n\n\n1.3.2 Choosing A likelihood\nIf we assume that these trials are independent. Meaning if its just me and a friend flipping coins we find in the street we can assume that these are independent. For this we can safely use the Binomial likelihood. To make this a little more concrete lets write out the likelihood with our toy example\n\\[\np(\\text{getting heads} | \\Theta) = \\frac{\\text{Number of Tosses}!}{\\text{getting heads}!(Tosses - \\text{getting heads})!}\\Theta^{\\text{Getting Heads}} (1 - \\Theta)^{\\text{Tosses - \\text{Getting Heads}}}\n\\]\n\nn_trials = [1,2, 5]\ngetting_heads = [0.25, .5, 0.75]\n\n\n_,axes = plt.subplots(3,3, figsize = (12,6), sharex = True, sharey = True)\nfor i, n in enumerate(n_trials):\n    for j, p in enumerate(getting_heads):\n        pz.Binomial(n, p).plot_pdf(legend = 'title', ax = axes[i, j])\n\n\n\n\n\n\n\n\nWhen we look at the plots as the number of trials grow we get more or less a probability of approx 50/50. When we change the probability to 0.5 and a t one trial it fits our mental model fairly well. While this is not always feasible it definitely helps.\n\n\n1.3.3 Choosing a prior\nA natural choice for our prior is the the beta distrbitution. It is bounded between 0 and 1 which helps in our coin flip problem but is especially helpful for proportions. We would have to play with our shape parameters a bit but. A mostly symetric shape around .5 makes the most sense a priori. We are not going to entirely rule out a biased coin in any direction but we feel pretty confident that over like 20 trials the proportion of heads is going to be closer to .5 than it is 1 or 0. The big benefit computationally is that the Beta distribution and the binomial are that it returns a posterior that is the same functional form of the prior making it easier to explore the posterior. This was a much bigger deal in olden times.2\n\n\n1.3.4 How to choose priors\nAs a Bayesian we do want the data to speak for itself. But we don‚Äôt have to pretend like we don‚Äôt know anything about the data. We really want to shoot for weakly informative priors. We know that a player or an offense generating close to 1 EPA/play is next to impossible in the NFL. Like it could theoretically happen but that would likely require us to take a top 5 offense in the NFL in the past 5-10 years and transport it back to when players were moonlighting in the offseason. However, we are probably not likely to see that or would not be a fun excercise to undertake. Instead we can impose some weakly informative priors to encourage our models to explore a reasonable set of bounds of the posterior distribution.\nOne helpful way we can start to do prior tuning is using preliz. In some respects we could theoretically just hand derive it but that is not a good use of our time. In the plot below we are just saying 90% of values should be between 0.1 and 0.7. This will then spit out the priors in the legend. We can then take this prior and feed it over to our modeling packages which is super nice. Since sometimes hand deriving logit priors and beta priors is supertedious.\n\ndist = pz.Beta()\n\npz.maxent(dist, 0.1, 0.7, 0.9)\n\n\npz.Beta(alpha = 2.47, beta = 3.61).to_pymc()\n\nbeta_rv{\"(),()-&gt;()\"}.out",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#excercises",
    "href": "thinking-probabilistically.html#excercises",
    "title": "1¬† Thinking Probabilistically",
    "section": "1.4 Excercises",
    "text": "1.4 Excercises\nThese will be a bit briefer than the ones in the book.\n\nUse PreliZ to explore different parameters for the BetaBinomial and Gaussian distribution.\n\n\n# _, ax = plt.subplots(2,2, figsize = (12,6), sharex  )",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#footnotes",
    "href": "thinking-probabilistically.html#footnotes",
    "title": "1¬† Thinking Probabilistically",
    "section": "",
    "text": "For reference the EPA/Play leader in 2024 was Jared Goff with 0.29 and the leader in 2023 was Brock Purdy with 0.338/‚Ü©Ô∏é\nBy olden times we are really talking closer to like your childhood.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#gaussians",
    "href": "Programming-Probabilistically.html#gaussians",
    "title": "2¬† Programming Probabilistically",
    "section": "2.3 Gaussians",
    "text": "2.3 Gaussians\nLets look at another toy example. Lets say that this is a reasonable model\n\\[\n\\begin{aligned}\n\\mu \\sim Uniform(l, h) \\\\\n\\sigma \\sim \\text{Half Normal}(\\sigma_\\sigma) \\\\\nY \\sim Normal(\\mu, \\sigma)\n\\end{aligned}\n\\]\nIn our case we are using a half normal which considers the absolute values of a noraml distribution to be centered around zero. So we are forcing our sigma to be positive because well it has to be.\n\n\nCode\nimport pandas as pd \n\nmolecules = np.loadtxt('https://raw.githubusercontent.com/aloctavodia/BAP3/refs/heads/main/code/data/chemical_shifts.csv')\n\n\nTo model it we can do this\n\nwith pm.Model() as first_lin_model:\n    mu = pm.Uniform(name = 'mu', lower = 40, upper = 70)\n    sigma = pm.HalfNormal(name = 'sigma', sigma = 5)\n    y = pm.Normal(name = 'Y',mu = mu, sigma = sigma, observed = molecules)\n    out_lin = pm.sample(1000, nuts_sampler='nutpie')\n\n\naz.plot_trace(out_lin)\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.16\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.13\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.11\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    1.09\n                    3\n                \n            \n            \n        \n    \n\n\n\narray([[&lt;Axes: title={'center': 'mu_interval__'}&gt;,\n        &lt;Axes: title={'center': 'mu_interval__'}&gt;],\n       [&lt;Axes: title={'center': 'sigma_log__'}&gt;,\n        &lt;Axes: title={'center': 'sigma_log__'}&gt;],\n       [&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'mu'}&gt;],\n       [&lt;Axes: title={'center': 'sigma'}&gt;,\n        &lt;Axes: title={'center': 'sigma'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThe diagnostic plots are looking pretty good. We can also plot the marginal distributions which I am not sure is the most informative plot for a lay audience.\n\naz.plot_pair(out_lin, kind = 'kde', marginals = True)\n\narray([[&lt;Axes: ylabel='mu_interval__'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='sigma_log__'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='mu'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: xlabel='mu_interval__', ylabel='sigma'&gt;,\n        &lt;Axes: xlabel='sigma_log__'&gt;, &lt;Axes: xlabel='mu'&gt;,\n        &lt;Axes: xlabel='sigma'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nWe have seen that we can grab some summary statistics with az.summary(out_lin) but it is not the nicest looking table. However we can pass her to GT like this:\n\nfrom great_tables import GT\nsumm_stats = az.summary(out_lin, kind = 'stats').round(2)\n\nGT(summ_stats)\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\n-0.2\n0.07\n-0.33\n-0.08\n\n\n1.26\n0.11\n1.05\n1.46\n\n\n53.5\n0.52\n52.54\n54.44\n\n\n3.54\n0.39\n2.84\n4.26\n\n\n\n\n\n\n\n\nWhich is slightly nicer than output of the arviz but the rownames don‚Äôt transfer over which is annoying. We do need a little bit of tinkering but for the most part I don‚Äôt really want to be in the table game all that often.\n\n2.3.1 Posterior predictive checks\nThe posterior predictive distribution is really just the distribution of future data given the model and observed data. Getting the posterior predictive is generally baked into all these Bayesian packages so we can really just do:\n\npm.sample_posterior_predictive(out_lin, model = first_lin_model, extend_inferencedata=True)\n\nSampling: [Y]\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 136kB\nDimensions:        (chain: 4, draw: 1000)\nCoordinates:\n  * chain          (chain) int64 32B 0 1 2 3\n  * draw           (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    mu_interval__  (chain, draw) float64 32kB -0.1811 -0.3029 ... -0.1935\n    sigma_log__    (chain, draw) float64 32kB 1.032 1.156 1.137 ... 1.238 1.197\n    mu             (chain, draw) float64 32kB 53.65 52.75 53.1 ... 53.32 53.55\n    sigma          (chain, draw) float64 32kB 2.807 3.177 3.118 ... 3.448 3.309\nAttributes:\n    created_at:                 2025-08-04T17:51:33.106163+00:00\n    arviz_version:              0.22.0\n    inference_library:          nutpie\n    inference_library_version:  0.15.2\n    sampling_time:              0.026218891143798828\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (4)mu_interval__(chain, draw)float64-0.1811 -0.3029 ... -0.2255 -0.1935array([[-0.18111325, -0.30289108, -0.25503653, ..., -0.19997461,\n        -0.23642029, -0.14997134],\n       [-0.13026834, -0.13026834, -0.15866522, ..., -0.31192329,\n        -0.13773521, -0.11360534],\n       [-0.20939705, -0.15839619, -0.21224385, ..., -0.27938192,\n        -0.0977094 , -0.28696581],\n       [-0.15111934, -0.11208521, -0.12278607, ..., -0.18836535,\n        -0.22551656, -0.1934501 ]], shape=(4, 1000))sigma_log__(chain, draw)float641.032 1.156 1.137 ... 1.238 1.197array([[1.03195505, 1.15599952, 1.13730486, ..., 1.18487561, 1.45250932,\n        1.08838327],\n       [1.30883888, 1.30883888, 1.2459621 , ..., 1.23251885, 1.35176186,\n        1.20944112],\n       [1.18794095, 1.29841493, 1.34163024, ..., 1.12811919, 1.32290738,\n        1.20666191],\n       [1.30169147, 1.27424089, 1.27554623, ..., 1.22721647, 1.23788005,\n        1.1966153 ]], shape=(4, 1000))mu(chain, draw)float6453.65 52.75 53.1 ... 53.32 53.55array([[53.64535152, 52.74552663, 53.09752684, ..., 53.50516859,\n        53.23506103, 53.87731839],\n       [54.02436674, 54.02436674, 53.81250107, ..., 52.67936063,\n        53.9686159 , 54.14887513],\n       [53.43523545, 53.81450616, 53.41411996, ..., 52.91815941,\n        54.26776195, 52.86240547],\n       [53.86875699, 54.16023992, 54.08025971, ..., 53.59142227,\n        53.31575783, 53.55363205]], shape=(4, 1000))sigma(chain, draw)float642.807 3.177 3.118 ... 3.448 3.309array([[2.80654741, 3.17719751, 3.11835264, ..., 3.27028001, 4.27382548,\n        2.96946935],\n       [3.70187289, 3.70187289, 3.47627771, ..., 3.42985797, 3.86422775,\n        3.35161099],\n       [3.2803199 , 3.66348517, 3.82527454, ..., 3.08983964, 3.75432075,\n        3.34230909],\n       [3.67550842, 3.57598582, 3.58065676, ..., 3.41171968, 3.44829548,\n        3.30889832]], shape=(4, 1000))Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (6)created_at :2025-08-04T17:51:33.106163+00:00arviz_version :0.22.0inference_library :nutpieinference_library_version :0.15.2sampling_time :0.026218891143798828tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:  (chain: 4, draw: 1000, Y_dim_0: 48)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * Y_dim_0  (Y_dim_0) int64 384B 0 1 2 3 4 5 6 7 8 ... 40 41 42 43 44 45 46 47\nData variables:\n    Y        (chain, draw, Y_dim_0) float64 2MB 55.66 51.38 ... 51.39 47.64\nAttributes:\n    created_at:                 2025-08-04T17:51:34.260241+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1xarray.DatasetDimensions:chain: 4draw: 1000Y_dim_0: 48Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Y_dim_0(Y_dim_0)int640 1 2 3 4 5 6 ... 42 43 44 45 46 47array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])Data variables: (1)Y(chain, draw, Y_dim_0)float6455.66 51.38 52.06 ... 51.39 47.64array([[[55.66300444, 51.37601914, 52.05777944, ..., 54.7370262 ,\n         50.90492849, 56.61333233],\n        [48.79970293, 57.20775538, 56.93463244, ..., 55.19878885,\n         50.15437595, 53.61181592],\n        [52.03552178, 53.35023455, 52.99575023, ..., 51.43698131,\n         46.04939664, 48.6897488 ],\n        ...,\n        [57.82888177, 48.04405569, 54.06644416, ..., 49.18091925,\n         51.56270459, 52.82614335],\n        [52.66677922, 53.66141567, 48.1018318 , ..., 50.37739375,\n         51.26475577, 48.63355286],\n        [49.96350898, 49.61205624, 54.81841993, ..., 56.11252007,\n         49.78408606, 54.57909887]],\n\n       [[54.93931698, 52.05883449, 51.51441658, ..., 52.99627845,\n         52.43707727, 50.44868179],\n        [46.36050035, 53.13646911, 50.59216868, ..., 56.82242858,\n         56.75850181, 56.22877888],\n        [61.93517462, 53.9424681 , 49.97238114, ..., 58.54448253,\n         53.46084727, 56.5214164 ],\n...\n        [57.07645621, 50.11902298, 52.29010231, ..., 49.87883069,\n         53.09831786, 51.77786347],\n        [65.81287258, 50.27910708, 55.56640279, ..., 47.14983569,\n         56.29423664, 59.62575501],\n        [49.01758477, 54.71404351, 49.26378444, ..., 51.36474795,\n         59.04876568, 48.56874619]],\n\n       [[58.5040845 , 53.50102364, 54.56580621, ..., 49.66278492,\n         58.39787032, 55.14667258],\n        [51.5997991 , 59.30397013, 47.12997928, ..., 53.25493427,\n         53.11349039, 51.19290878],\n        [48.77698372, 49.91081046, 55.97107658, ..., 59.94027783,\n         51.28461227, 60.63706219],\n        ...,\n        [51.92377477, 54.42170585, 56.75379259, ..., 58.39726187,\n         59.72339818, 57.02452701],\n        [46.95937333, 47.26198835, 59.02583617, ..., 49.83424154,\n         52.19816539, 53.59389849],\n        [54.47303308, 50.02179435, 57.27805218, ..., 52.1158826 ,\n         51.38993302, 47.6377445 ]]], shape=(4, 1000, 48))Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Y_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n      dtype='int64', name='Y_dim_0'))Attributes: (4)created_at :2025-08-04T17:51:34.260241+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 336kB\nDimensions:               (chain: 4, draw: 1000)\nCoordinates:\n  * chain                 (chain) int64 32B 0 1 2 3\n  * draw                  (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables:\n    depth                 (chain, draw) uint64 32kB 2 2 1 2 1 2 ... 1 2 1 1 2 2\n    maxdepth_reached      (chain, draw) bool 4kB False False ... False False\n    index_in_trajectory   (chain, draw) int64 32kB 1 -1 1 -1 -1 ... -3 1 1 2 2\n    logp                  (chain, draw) float64 32kB -131.7 -131.0 ... -129.4\n    energy                (chain, draw) float64 32kB 132.7 132.6 ... 129.4 129.6\n    diverging             (chain, draw) bool 4kB False False ... False False\n    energy_error          (chain, draw) float64 32kB 0.3398 0.003214 ... 0.00456\n    step_size             (chain, draw) float64 32kB 1.156 1.156 ... 1.086 1.086\n    step_size_bar         (chain, draw) float64 32kB 1.156 1.156 ... 1.086 1.086\n    mean_tree_accept      (chain, draw) float64 32kB 0.6301 0.8451 ... 0.9486\n    mean_tree_accept_sym  (chain, draw) float64 32kB 0.7703 0.9103 ... 0.9733\n    n_steps               (chain, draw) uint64 32kB 3 3 1 3 3 3 ... 1 3 3 3 3 3\nAttributes:\n    created_at:     2025-08-04T17:51:33.099823+00:00\n    arviz_version:  0.22.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (12)depth(chain, draw)uint642 2 1 2 1 2 2 1 ... 1 2 1 2 1 1 2 2array([[2, 2, 1, ..., 1, 2, 2],\n       [2, 1, 1, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2],\n       [1, 1, 1, ..., 1, 2, 2]], shape=(4, 1000), dtype=uint64)maxdepth_reached(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))index_in_trajectory(chain, draw)int641 -1 1 -1 -1 -2 3 ... -1 -3 1 1 2 2array([[ 1, -1,  1, ...,  0, -1,  2],\n       [-2,  0,  1, ...,  1, -2,  1],\n       [ 2,  3,  2, ...,  2,  2,  3],\n       [-1,  1,  1, ...,  1,  2,  2]], shape=(4, 1000))logp(chain, draw)float64-131.7 -131.0 ... -129.4 -129.4array([[-131.70430597, -131.03640815, -130.25525087, ..., -129.49171105,\n        -131.43604537, -130.8965513 ],\n       [-130.09673548, -130.09673548, -129.56643953, ..., -130.74288508,\n        -130.32428733, -130.30111256],\n       [-129.48582266, -129.72753288, -129.88623999, ..., -130.80637895,\n        -130.72049016, -130.28013901],\n       [-129.81147121, -130.25769031, -130.07416339, ..., -129.38672417,\n        -129.43517654, -129.44580683]], shape=(4, 1000))energy(chain, draw)float64132.7 132.6 131.1 ... 129.4 129.6array([[132.65402531, 132.57156397, 131.08560111, ..., 131.43053939,\n        132.2343544 , 131.40582593],\n       [131.63815233, 133.7921702 , 129.99546596, ..., 131.1878092 ,\n        131.0462909 , 130.89503191],\n       [129.60986691, 130.14880448, 130.66433519, ..., 132.50529731,\n        131.00559641, 130.69894736],\n       [131.8613351 , 130.33742487, 130.37095669, ..., 130.70398027,\n        129.44704569, 129.6452914 ]], shape=(4, 1000))diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))energy_error(chain, draw)float640.3398 0.003214 ... 0.01531 0.00456array([[ 0.339834  ,  0.0032141 , -0.38097098, ...,  0.        ,\n         0.76055461, -0.14149191],\n       [-0.00458091,  0.        , -0.13900006, ...,  0.04585172,\n        -0.16089623,  0.0603784 ],\n       [-0.01160545,  0.09983363,  0.05248158, ...,  0.35916224,\n        -0.09458837, -0.16399197],\n       [-0.6264024 ,  0.13046131, -0.05429575, ..., -0.45719588,\n         0.01531164,  0.00456002]], shape=(4, 1000))step_size(chain, draw)float641.156 1.156 1.156 ... 1.086 1.086array([[1.15607139, 1.15607139, 1.15607139, ..., 1.15607139, 1.15607139,\n        1.15607139],\n       [1.1340138 , 1.1340138 , 1.1340138 , ..., 1.1340138 , 1.1340138 ,\n        1.1340138 ],\n       [1.11175393, 1.11175393, 1.11175393, ..., 1.11175393, 1.11175393,\n        1.11175393],\n       [1.08562531, 1.08562531, 1.08562531, ..., 1.08562531, 1.08562531,\n        1.08562531]], shape=(4, 1000))step_size_bar(chain, draw)float641.156 1.156 1.156 ... 1.086 1.086array([[1.15607139, 1.15607139, 1.15607139, ..., 1.15607139, 1.15607139,\n        1.15607139],\n       [1.1340138 , 1.1340138 , 1.1340138 , ..., 1.1340138 , 1.1340138 ,\n        1.1340138 ],\n       [1.11175393, 1.11175393, 1.11175393, ..., 1.11175393, 1.11175393,\n        1.11175393],\n       [1.08562531, 1.08562531, 1.08562531, ..., 1.08562531, 1.08562531,\n        1.08562531]], shape=(4, 1000))mean_tree_accept(chain, draw)float640.6301 0.8451 1.0 ... 0.9926 0.9486array([[0.63010778, 0.84513791, 1.        , ..., 0.21940682, 0.51095818,\n        1.        ],\n       [0.67681824, 0.64683584, 1.        , ..., 0.98506119, 1.        ,\n        0.98046941],\n       [0.98949324, 0.85704695, 0.87830199, ..., 0.56466432, 1.        ,\n        1.        ],\n       [1.        , 0.87769045, 1.        , ..., 0.88559686, 0.99256226,\n        0.94857558]], shape=(4, 1000))mean_tree_accept_sym(chain, draw)float640.7703 0.9103 ... 0.9956 0.9733array([[0.77034143, 0.91031992, 0.81178546, ..., 0.35985828, 0.6342917 ,\n        0.84635787],\n       [0.77985559, 0.62383491, 0.95070345, ..., 0.96942518, 0.88648991,\n        0.96216906],\n       [0.99277067, 0.92198303, 0.93282504, ..., 0.71686585, 0.85426729,\n        0.91478706],\n       [0.77711114, 0.93486171, 0.97285879, ..., 0.77738166, 0.99564969,\n        0.9732544 ]], shape=(4, 1000))n_steps(chain, draw)uint643 3 1 3 3 3 3 1 ... 3 3 1 3 3 3 3 3array([[3, 3, 1, ..., 1, 3, 3],\n       [3, 3, 3, ..., 3, 3, 3],\n       [3, 3, 3, ..., 3, 3, 3],\n       [3, 1, 1, ..., 3, 3, 3]], shape=(4, 1000), dtype=uint64)Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (2)created_at :2025-08-04T17:51:33.099823+00:00arviz_version :0.22.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 768B\nDimensions:  (Y_dim_0: 48)\nCoordinates:\n  * Y_dim_0  (Y_dim_0) int64 384B 0 1 2 3 4 5 6 7 8 ... 40 41 42 43 44 45 46 47\nData variables:\n    Y        (Y_dim_0) float64 384B 51.06 55.12 53.73 50.24 ... 54.3 53.84 53.16\nAttributes:\n    created_at:                 2025-08-04T17:51:33.105744+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1xarray.DatasetDimensions:Y_dim_0: 48Coordinates: (1)Y_dim_0(Y_dim_0)int640 1 2 3 4 5 6 ... 42 43 44 45 46 47array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])Data variables: (1)Y(Y_dim_0)float6451.06 55.12 53.73 ... 53.84 53.16array([51.06, 55.12, 53.73, 50.24, 52.05, 56.4 , 48.45, 52.34, 55.65,\n       51.49, 51.86, 63.43, 53.  , 56.09, 51.93, 52.31, 52.33, 57.48,\n       57.44, 55.14, 53.93, 54.62, 56.09, 68.58, 51.36, 55.47, 50.73,\n       51.94, 54.95, 50.39, 52.91, 51.5 , 52.68, 47.72, 49.73, 51.82,\n       54.99, 52.84, 53.19, 54.52, 51.46, 53.73, 51.61, 49.81, 52.42,\n       54.3 , 53.84, 53.16])Indexes: (1)Y_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n      dtype='int64', name='Y_dim_0'))Attributes: (4)created_at :2025-08-04T17:51:33.105744+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1\n                      \n                  \n            \n            \n            \n                  \n                  warmup_posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 136kB\nDimensions:        (chain: 4, draw: 1000)\nCoordinates:\n  * chain          (chain) int64 32B 0 1 2 3\n  * draw           (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    mu_interval__  (chain, draw) float64 32kB -0.8812 -0.8812 ... -0.03508\n    sigma_log__    (chain, draw) float64 32kB 1.157 1.157 1.256 ... 1.16 1.346\n    mu             (chain, draw) float64 32kB 48.79 48.79 50.62 ... 52.51 54.74\n    sigma          (chain, draw) float64 32kB 3.182 3.182 3.51 ... 3.19 3.844\nAttributes:\n    created_at:     2025-08-04T17:51:33.097739+00:00\n    arviz_version:  0.22.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (4)mu_interval__(chain, draw)float64-0.8812 -0.8812 ... -0.03508array([[-0.88123727, -0.88123727, -0.60123624, ..., -0.18750795,\n        -0.26023412, -0.119493  ],\n       [ 0.23348665,  0.23348665,  0.13898462, ..., -0.16302041,\n        -0.16302041, -0.24586623],\n       [-0.94311512, -0.94311512, -0.87877735, ..., -0.25810008,\n        -0.15150641, -0.17051098],\n       [ 0.77755586,  0.77755586,  0.75571055, ..., -0.18264793,\n        -0.33464065, -0.03508347]], shape=(4, 1000))sigma_log__(chain, draw)float641.157 1.157 1.256 ... 1.16 1.346array([[1.15737418, 1.15737418, 1.25556962, ..., 1.13610648, 1.242989  ,\n        1.31206022],\n       [2.34781007, 2.34781007, 2.32069013, ..., 1.43392242, 1.43392242,\n        1.14959543],\n       [1.30644241, 1.30644241, 1.36925764, ..., 1.24220595, 1.25814599,\n        1.26906725],\n       [2.06848943, 2.06848943, 1.99357118, ..., 1.44810327, 1.1601746 ,\n        1.34648462]], shape=(4, 1000))mu(chain, draw)float6448.79 48.79 50.62 ... 52.51 54.74array([[48.78764355, 48.78764355, 50.62182735, ..., 53.59779637,\n        53.05918475, 54.10486736],\n       [56.74323756, 56.74323756, 56.04070997, ..., 53.78004745,\n        53.78004745, 53.16523666],\n       [48.40814595, 48.40814595, 48.80293616, ..., 53.07492423,\n        53.86587052, 53.72425707],\n       [60.55459323, 60.55459323, 60.4126517 , ..., 53.63393607,\n        52.51335734, 54.73690098]], shape=(4, 1000))sigma(chain, draw)float643.182 3.182 3.51 ... 3.19 3.844array([[ 3.18156808,  3.18156808,  3.50983709, ...,  3.1146179 ,\n         3.46595773,  3.71381711],\n       [10.46263216, 10.46263216, 10.18269927, ...,  4.19512201,\n         4.19512201,  3.15691545],\n       [ 3.69301208,  3.69301208,  3.93243034, ...,  3.46324478,\n         3.51889136,  3.55753272],\n       [ 7.91286112,  7.91286112,  7.34170554, ...,  4.25503622,\n         3.19049028,  3.84388901]], shape=(4, 1000))Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (2)created_at :2025-08-04T17:51:33.097739+00:00arviz_version :0.22.0\n                      \n                  \n            \n            \n            \n                  \n                  warmup_sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 336kB\nDimensions:               (chain: 4, draw: 1000)\nCoordinates:\n  * chain                 (chain) int64 32B 0 1 2 3\n  * draw                  (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables:\n    depth                 (chain, draw) uint64 32kB 2 0 2 1 1 1 ... 2 2 2 2 2 2\n    maxdepth_reached      (chain, draw) bool 4kB False False ... False False\n    index_in_trajectory   (chain, draw) int64 32kB 0 0 2 -1 0 0 ... 2 -3 2 2 -2\n    logp                  (chain, draw) float64 32kB -182.4 -182.4 ... -132.4\n    energy                (chain, draw) float64 32kB 184.7 182.5 ... 133.7 132.5\n    diverging             (chain, draw) bool 4kB False True ... False False\n    energy_error          (chain, draw) float64 32kB 0.0 0.0 ... 0.4697 0.05577\n    step_size             (chain, draw) float64 32kB 3.736 0.3684 ... 1.086\n    step_size_bar         (chain, draw) float64 32kB 3.736 0.9423 ... 1.086\n    mean_tree_accept      (chain, draw) float64 32kB 1.111e-08 0.0 ... 0.9819\n    mean_tree_accept_sym  (chain, draw) float64 32kB 2.222e-08 0.0 ... 0.7951\n    n_steps               (chain, draw) uint64 32kB 7 1 3 1 1 2 ... 3 3 3 3 3 3\nAttributes:\n    created_at:     2025-08-04T17:51:33.102259+00:00\n    arviz_version:  0.22.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (12)depth(chain, draw)uint642 0 2 1 1 1 1 3 ... 1 1 2 2 2 2 2 2array([[2, 0, 2, ..., 2, 2, 2],\n       [1, 0, 2, ..., 2, 1, 2],\n       [2, 0, 1, ..., 2, 2, 1],\n       [2, 1, 3, ..., 2, 2, 2]], shape=(4, 1000), dtype=uint64)maxdepth_reached(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))index_in_trajectory(chain, draw)int640 0 2 -1 0 0 1 ... 1 3 2 -3 2 2 -2array([[ 0,  0,  2, ..., -2,  3, -2],\n       [ 0,  0,  1, ..., -1,  0,  2],\n       [ 0,  0,  1, ...,  1, -2, -1],\n       [ 0,  0,  6, ...,  2,  2, -2]], shape=(4, 1000))logp(chain, draw)float64-182.4 -182.4 ... -131.9 -132.4array([[-182.42156894, -182.42156894, -145.56407432, ..., -129.89291301,\n        -129.75804215, -130.27372664],\n       [-164.75614836, -164.75614836, -162.68722305, ..., -131.14377622,\n        -131.14377622, -130.00705722],\n       [-175.36490366, -175.36490366, -164.5239296 , ..., -129.73076385,\n        -129.65141328, -129.51581173],\n       [-169.52782655, -169.52782655, -168.75255993, ..., -131.29144403,\n        -131.9461628 , -132.40917252]], shape=(4, 1000))energy(chain, draw)float64184.7 182.5 181.9 ... 133.7 132.5array([[184.67494057, 182.50089395, 181.88068938, ..., 130.4925054 ,\n        130.1253155 , 131.15273872],\n       [164.79268614, 165.16019774, 165.01790385, ..., 131.20664351,\n        134.77723642, 130.89859304],\n       [175.52382917, 175.59550535, 174.07961604, ..., 130.15216496,\n        129.7454928 , 129.6437538 ],\n       [170.13067873, 169.928445  , 170.92155685, ..., 133.35278621,\n        133.72522917, 132.53465421]], shape=(4, 1000))diverging(chain, draw)boolFalse True False ... False Falsearray([[False,  True, False, ..., False, False, False],\n       [ True,  True, False, ..., False, False, False],\n       [ True,  True, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))energy_error(chain, draw)float640.0 0.0 -2.969 ... 0.4697 0.05577array([[ 0.        ,  0.        , -2.96865976, ..., -0.33283764,\n        -0.04856957,  0.36374138],\n       [ 0.        ,  0.        , -0.02057246, ...,  0.33098546,\n         0.        , -0.33192679],\n       [ 0.        ,  0.        , -1.29815385, ...,  0.08611986,\n        -0.02956323, -0.05448171],\n       [ 0.        ,  0.        ,  0.01510536, ...,  0.05705192,\n         0.46972789,  0.05577083]], shape=(4, 1000))step_size(chain, draw)float643.736 0.3684 0.3837 ... 1.241 1.086array([[3.73610374, 0.36837736, 0.38366626, ..., 1.32727061, 1.4818215 ,\n        1.15607139],\n       [3.73610367, 0.36837736, 0.38366625, ..., 1.3332311 , 1.04919687,\n        1.1340138 ],\n       [3.73610367, 0.36837736, 0.38366625, ..., 1.12358835, 1.25705044,\n        1.11175393],\n       [0.95137764, 0.09431832, 0.09454076, ..., 1.23322133, 1.24139037,\n        1.08562531]], shape=(4, 1000))step_size_bar(chain, draw)float643.736 0.9423 0.6353 ... 1.085 1.086array([[3.73610374, 0.94226451, 0.63531197, ..., 1.15365652, 1.15528784,\n        1.15607139],\n       [3.73610367, 0.94226449, 0.63531196, ..., 1.13474281, 1.1342405 ,\n        1.1340138 ],\n       [3.73610367, 0.94226449, 0.63531196, ..., 1.10950448, 1.11028669,\n        1.11175393],\n       [0.95137764, 0.24072186, 0.1597542 , ..., 1.08397913, 1.08480909,\n        1.08562531]], shape=(4, 1000))mean_tree_accept(chain, draw)float641.111e-08 0.0 1.0 ... 0.7153 0.9819array([[1.11082919e-08, 0.00000000e+00, 1.00000000e+00, ...,\n        1.00000000e+00, 9.66423013e-01, 4.47353403e-01],\n       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        9.06071870e-01, 5.57128249e-01, 1.00000000e+00],\n       [2.64297469e-14, 0.00000000e+00, 1.00000000e+00, ...,\n        9.05558222e-01, 1.00000000e+00, 1.00000000e+00],\n       [1.01237947e-02, 2.86647392e-79, 9.84454360e-01, ...,\n        7.45464329e-01, 7.15339525e-01, 9.81918615e-01]], shape=(4, 1000))mean_tree_accept_sym(chain, draw)float642.222e-08 0.0 ... 0.808 0.7951array([[2.22165831e-08, 0.00000000e+00, 2.95944570e-01, ...,\n        8.48016030e-01, 9.72970716e-01, 5.92438874e-01],\n       [0.00000000e+00, 0.00000000e+00, 9.55900465e-01, ...,\n        8.98647614e-01, 4.16577604e-01, 8.65606561e-01],\n       [5.28594939e-14, 0.00000000e+00, 4.28951769e-01, ...,\n        9.48160748e-01, 9.76174497e-01, 9.72765879e-01],\n       [1.96518545e-02, 5.73294783e-79, 9.92087185e-01, ...,\n        8.37047590e-01, 8.07962353e-01, 7.95091388e-01]], shape=(4, 1000))n_steps(chain, draw)uint647 1 3 1 1 2 1 7 ... 3 3 3 3 3 3 3 3array([[7, 1, 3, ..., 3, 3, 3],\n       [2, 1, 3, ..., 3, 3, 3],\n       [7, 1, 1, ..., 3, 3, 1],\n       [3, 1, 7, ..., 3, 3, 3]], shape=(4, 1000), dtype=uint64)Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (2)created_at :2025-08-04T17:51:33.102259+00:00arviz_version :0.22.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nThen we can eyeball how good the predictions match the observed data by doing:\n\naz.plot_ppc(out_lin, num_pp_samples=100, figsize=(12,4))\n\n\n\n\n\n\n\n\nFor a model with not greatish priors It does kind of okay. The extreme observations in the data are kind of throwing off the predictions. If we were wanted to rip this off and hope that we get more data and that the model will look better with more data we are welcome to do that. However, if this is the data we need to account for these points with a little bit more robust of a prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#footnotes",
    "href": "Programming-Probabilistically.html#footnotes",
    "title": "2¬† Programming Probabilistically",
    "section": "",
    "text": "If I am understanding the whole bit of Bayesianism.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#robust-regression",
    "href": "Programming-Probabilistically.html#robust-regression",
    "title": "2¬† Programming Probabilistically",
    "section": "2.4 Robust regression",
    "text": "2.4 Robust regression\nBy default with our OLS we are making an assumption about what the distribution looks like. In this case we are making the assumption that the data are normally distributed. This works a lot of the time but one of the core problems we are running into is that with a normal we are making the assumption that there are not a lot of data in the tails.\n\nig, ax = plt.subplots()\nax.hist(molecules, bins=20, edgecolor='black', alpha=0.6)\n\n\nrug_height = 0.05  \nymin, ymax = ax.get_ylim()\nrug_ymin = ymin\nrug_ymax = ymin + (ymax - ymin) * rug_height\n\nax.vlines(molecules, rug_ymin, rug_ymax, color='black', alpha=0.7)\n\n\n\n\n\n\n\n\nWe see some data in the tails. We could exclude them but generally this is probably not a good idea. There is nothing really wrong with the data unless we have reasons to suspect that there were data collection errors. It is generally better practice, in Bayesianism, to make our model more robust than to throw away information.\n\n2.4.1 Degrees of normality\nOne of the nice things about stats is there are lots of problems that arise in lots of fields that people have had to deal with. One of the first one is how do we deal with deviations from the normal. Versions of the Student-T distribution arise a bit earlier than the Biometrika paper that introduces it in the English language.\nThe history of the distribution is fun in of its self. William Sealy Gosset AKA Student was working at the Guinness Brewery working on small sample problems. You would like to assess the quality of ingredients or the quality of the beer without having to take a ton of units off the line. The more you sample the more cost you are incuring. So kind of by design the distribution can handle data in the tails.\nFor a Student prior we can tune it in a variety of ways but in general we are really looking to make an assumption about how much stuff is in the tails.\n\nfor nu in [1, 2, 10]:\n    pz.StudentT(nu, 0, 1).plot_pdf(support=(-5, 5), figsize=(12, 4))\n\nax = pz.StudentT(np.inf, 0, 1).plot_pdf(support=(-5, 5), figsize=(12, 4), color=\"k\")\nax.get_lines()[-1].set_linestyle(\"--\")\n\n\n\n\n\n\n\n\nIn general when we have smaller values than the more mass we have in the tails in the distribution. Roughly this just means that the heavier the tails are the less surprised we would be to find a point like 2 standard deviations away from the mean.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#a-robust-version-of-the-normal-model",
    "href": "Programming-Probabilistically.html#a-robust-version-of-the-normal-model",
    "title": "2¬† Programming Probabilistically",
    "section": "2.5 A robust version of the normal model",
    "text": "2.5 A robust version of the normal model\nSo we are going to rewrite the same model just with the addition of a student prior and an exponential prior. So the model looks a bit different becuase we have to constrain the weights of the tails to be strictly positive. A really common prior for the this is the exponential prior. \\[\n\\begin{aligned}\n\\mu \\sim Uniform(l, h) \\\\\n\\sigma \\sim \\text{Half Normal}(\\sigma_\\sigma) \\\\\n\\nu \\sim Exponential(\\lambda) \\\\\nY \\sim \\text{Student-T}(\\nu, \\mu, \\sigma)\n\\end{aligned}\n\\] Really any prior that constrains the values of \\(\\nu\\) to be strictly positive would work. One thing that is a bit tricky about the exponential is that by default we are going to get the inverse of the mean.\n\nwith pm.Model() as model_t:\n    mu = pm.Uniform(name = 'mu', lower = 40, upper = 70)\n    nu = pm.Exponential('nu', 1/30)\n    sigma = pm.HalfNormal(name = 'sigma', sigma = 10)\n    y = pm.StudentT(name = 'Y',mu = mu, sigma = sigma, nu = nu, observed = molecules)\n    out_t_mod = pm.sample(1000, nuts_sampler='nutpie') \n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.89\n                    5\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.91\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.88\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.91\n                    3\n                \n            \n            \n        \n    \n\n\n\nThe exponential part is really grafting onto the tail of distribution. We are setting the ‚Äúmean‚Äù around 30 which makes it mimic a Gaussian distribution. We are not forcing the model to be a strictly Gaussian or a strictly Student-T(?) but let it fall back on the Student-T distribution when needed. We are effecitvely saying that on average we expect 30 molecule somethings, but am open to the idea that there could be some events that happen that are big but these are not likely. When we look at the posterior predictive we see that it looks much better.\n\npm.sample_posterior_predictive(out_t_mod, model = model_t, extend_inferencedata=True)\n\nax = az.plot_ppc(out_t_mod, figsize=(12,4), num_pp_samples=100)\nax.set_xlim(40,70)\n\nSampling: [Y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.1 Misc posterior stuff\nThere are lot of ways to manipulate the posterior\n\nposterior = out_t_mod.posterior\n\nstack = az.extract(out_t_mod)\n\nWe are going to play around with it more a bit later.\n\n\n2.5.2 Groups comparision\nOften times we need to compare groups. Whether this is treatment and control groups, gender, or regions. One thing that really helps in a Bayesian context is to use contrast coding. In the book they use Pandas but that‚Äôs never been the API of choice for me. We could to some preprocessing along these lines.\n\nimport catfact as fct\n\ntips = pl.read_csv('https://raw.githubusercontent.com/aloctavodia/BAP3/refs/heads/main/code/data/tips.csv')\n\ncategories = np.array([\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n\nday_enum = pl.Enum(categories)\n\n# Cast the 'day' column to Enum and get the physical codes\ntips = tips.with_columns(\n    pl.col(\"day\").cast(day_enum).alias(\"day_enum\")\n)\nidx = tips[\"day_enum\"].to_physical().to_numpy()\ntip = tips[\"tip\"]\n\nwith pm.Model() as cat_model:\n    mu = pm.Normal('mu', mu = 0, sigma = 10, shape = 4)\n    sigma = pm.HalfNormal('sigma', sigma = 10, shape = 4)\n    y = pm.Normal('y', mu = mu[idx], sigma = sigma[idx], observed= tip)\n\nHowever, PyMc includes a coordinates and dimensions which is really just a way of saying we can feed a dicitonary with our original variable and then our flattened variable for modeling. This just has the benefit of working better when visualizing things later.\n\ncoords = {\"days\": categories, \"days_flat\": categories[idx]}\n\nwith pm.Model(coords = coords) as comparing_groups:\n    mu = pm.HalfNormal(\"mu\", sigma=5, dims=\"days\")\n    sigma = pm.HalfNormal(\"sigma\", sigma=1, dims=\"days\")\n   \n    y = pm.Gamma(\"y\", mu=mu[idx], sigma=sigma[idx], observed=tip, dims=\"days_flat\") \n    cat_data_out = pm.sample()\n    cat_data_out.extend(pm.sample_posterior_predictive(cat_data_out))     \n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\nSo now when we co to plot it we can just extract it out with easy labels like this:\n\n_, axes = plt.subplots(2,2)\n\naz.plot_ppc(cat_data_out, num_pp_samples=100, coords = {\"days_flat\" : [categories]}, flatten=[], ax = axes)\n\narray([&lt;Axes: xlabel='y\\nSun'&gt;, &lt;Axes: xlabel='y\\nSat'&gt;,\n       &lt;Axes: xlabel='y\\nThur'&gt;, &lt;Axes: xlabel='y\\nFri'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nWe could use our trusty dusty Cohen‚Äôs d or probability of superiority but a better way to do this is to use contrasts.\n\ncat_posterior = az.extract(cat_data_out)\n\ndist = pz.Normal(0,1)\n\ncomparisions = [(categories[i], categories[j]) for i in range(4) for j in range(i+1, 4)]\n\n_, axes = plt.subplots(3,2, figsize = (12,6), sharex=True)\n\nfor(i,j), ax in zip(comparisions, axes.ravel()): \n    mean_diff = cat_posterior['mu'].sel(days = i) - cat_posterior['mu'].sel(days = j)\n\n    cohen_d = (mean_diff /\n               np.sqrt((cat_posterior['sigma'].sel(days = i)**2 +\n                        cat_posterior['sigma'].sel(days = j)**2) / 2)\n    ).mean().item()\n    ps = dist.cdf(cohen_d/(2**0.5))\n    az.plot_posterior(mean_diff.values, ref_val=0, ax = ax)\n    ax.set_title(f\"{i} - {j}\")\n    ax.plot(0, label=f\"Cohen's d = {cohen_d:.2f}\\nProb sup = {ps:.2f}\", alpha=0)\n    ax.legend(loc=1)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  },
  {
    "objectID": "mlm.html",
    "href": "mlm.html",
    "title": "3¬† Hierarchical Models",
    "section": "",
    "text": "3.1 Sharing information, sharing priors\nThese are a major workhorse in applied work because they are super flexible in a Bayesian framework and get us really interesting insights. We can integrate all kinds of levels add more and more information into our models.\nOne of the nice parts of MLM‚Äôs are that you can share information. We can learn something about individual level coffee shops by borrowing information from a global trend and/or individual features.\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl \nimport polars.selectors as cs\nimport preliz as pz\nimport pymc as pm\n\nchemical_data = pl.read_csv('https://raw.githubusercontent.com/aloctavodia/BAP3/refs/heads/main/code/data/chemical_shifts_theo_exp.csv')\n\ndiff = chemical_data.with_columns(\n    (pl.col('theo') - pl.col('exp')).alias('diff')\n)\n\ncat_encode  = chemical_data['aa'].cast(pl.Categorical)\n\nidx = cat_encode.to_physical().to_numpy()\n\ncoords = {'aa' : cat_encode.cat.get_categories()}\nSo now lets write our no pooling model. In Pols and Economics we are effectively making a fixed intercept model where each aa gets its own intercept.\nwith pm.Model(coords = coords) as cs_nh:\n    mu = pm.Normal('mu', mu = 0, sigma = 10, dims = 'aa')\n    sigma = pm.HalfNormal('sigma', sigma = 10, dims = 'aa')\n    y = pm.Normal('y', mu = mu[idx], sigma = sigma[idx], observed=diff['diff'])\n    out_cs_nh = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nWith the MLM what we are going to do is let it share some information or use partial pooliing to be a bit more formal. We are going to include ‚Äúhyper priors‚Äù or basically we are going to learn optimal values for our global mean and standard deviation. The model does not actually differ all that. The real difference comes from the fact that we are passing off the hyperpriors to our varying slopes and varying intercepts.\nwith pm.Model(coords = coords) as first_mlm:\n    # hyper priors \n    gb_mu = pm.Normal('gb_mu', mu = 0, sigma = 10)\n    gb_sigma = pm.HalfNormal('gb_sd', 10)\n\n\n    mu = pm.Normal('mu', mu = gb_mu, sigma = gb_sigma, dims = 'aa')\n    sigma = pm.HalfNormal('sigma', sigma = 10, dims = 'aa')\n    y = pm.Normal('y', mu = mu[idx], sigma = sigma[idx], observed = diff['diff'])\n    out_first_mlm = pm.sample(nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.61\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.61\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.61\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.61\n                    7\nOne of the nice built in things in Arviz is that we can compare our model estimates using plot_forest\naxes = az.plot_forest([out_cs_nh, out_first_mlm], model_names=['non-hierarchical', 'hierarichical'],\n                       var_names='mu', combined = True, r_hat = False, ess = False, figsize=(10,7))\n\ny_lims = axes[0].get_ylim()\n\naxes[0].vlines(out_first_mlm.posterior['gb_mu'].mean(), *y_lims, color = 'k', ls = \":\")\nOne thing that its a little hard to notice right off the bat, but is kind of the point, is that in the hierarchical model the estimates are pulled closer to the ‚Äúglobal mean‚Äù in almost every case. For the most part we do see that for the most part we aren‚Äôt buying a ton of leverage for some groups, but for some groups like PRO or TYR there is an appreciable difference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "mlm.html#water-quality",
    "href": "mlm.html#water-quality",
    "title": "3¬† Hierarchical Models",
    "section": "3.2 Water Quality",
    "text": "3.2 Water Quality\nHere we are just going to sim up some data. The data consists of three groups consisting of some number of samples that record the quality of the waters in each group.\n\nN_samples = [30, 30, 30]\nG_samples = [18, 18, 18]\ngroup_idx = np.repeat(np.arange(len(N_samples)), N_samples)\ndata = []\nfor i in range(0, len(N_samples)):\n    data.extend(np.repeat([1, 0], [G_samples[i], N_samples[i]-G_samples[i]]))\n\nWe are going to define a beta hierarchical model that follows this form\n\\[\n\\begin{aligned}\n\\mu \\sim Beta(\\alpha_\\mu, \\beta_\\mu) \\\\\n\\nu \\sim \\text{Half Normal}(\\sigma_\\nu) \\\\\n\\Theta_i \\sim Beta(\\mu, \\nu) \\\\\nY_i \\sim Bernoulli(\\theta_i)\n\\end{aligned}\n\\]\nNow to express it as a mode we are going to rexpress it like this\n\nwith pm.Model() as beta_hm:\n    # pretty symmetric\n    mu = pm.Beta('mu', 1.0, 1.0)\n    nu = pm.HalfNormal('nu', 10)\n    theta = pm.Beta('theta', mu = mu, nu = nu, shape = len(N_samples))\n    y = pm.Bernoulli('y',theta[group_idx], observed = data)\n    out_beta_hm = pm.sample(nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.79\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.77\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.73\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.74\n                    7\n                \n            \n            \n        \n    \n\n\n\nNow lets take a look at her\n\naz.plot_trace(out_beta_hm)\n\narray([[&lt;Axes: title={'center': 'mu_logodds__'}&gt;,\n        &lt;Axes: title={'center': 'mu_logodds__'}&gt;],\n       [&lt;Axes: title={'center': 'nu_log__'}&gt;,\n        &lt;Axes: title={'center': 'nu_log__'}&gt;],\n       [&lt;Axes: title={'center': 'theta_logodds__'}&gt;,\n        &lt;Axes: title={'center': 'theta_logodds__'}&gt;],\n       [&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'mu'}&gt;],\n       [&lt;Axes: title={'center': 'nu'}&gt;, &lt;Axes: title={'center': 'nu'}&gt;],\n       [&lt;Axes: title={'center': 'theta'}&gt;,\n        &lt;Axes: title={'center': 'theta'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nWe are being implored to change up the model to set the g samples to switch up the number of samples\n\ng_samp18 =  az.summary(out_beta_hm, kind = 'stats')\n\n\nN_samples = [30, 30, 30]\nG_samples = [13, 13, 13]\ngroup_idx = np.repeat(np.arange(len(N_samples)), N_samples)\ndata13 = []\nfor i in range(0, len(N_samples)):\n    data13.extend(np.repeat([1, 0], [G_samples[i], N_samples[i]-G_samples[i]]))\n\n\nwith pm.Model() as pm_mod13: \n    mu = pm.Beta('mu', 1.0, 1.0)\n    nu = pm.HalfNormal('nu', sigma = 10)\n    theta = pm.Beta('theta', mu = mu, nu = nu, shape = len(N_samples))\n    y = pm.Bernoulli('y', theta[group_idx], observed = data13)\n    out_beta_13 = pm.sample(nuts_sampler='nutpie')\n\ngsamp13 = az.summary(out_beta_13, kind = 'stats')\n\n\nN_samples = [30, 30, 30]\nG_samples = [18, 3, 3]\ngroup_idx = np.repeat(np.arange(len(N_samples)), N_samples)\ndata_mix = []\nfor i in range(0, len(N_samples)):\n    data_mix.extend(np.repeat([1, 0], [G_samples[i], N_samples[i]-G_samples[i]]))\n\nwith pm.Model() as pm_mix_samp:\n    mu = pm.Beta('mu', 1.0, 1.0)\n    nu = pm.HalfNormal('nu', 10)\n    theta = pm.Beta('theta', mu = mu, nu = nu, shape = len(N_samples))\n    y = pm.Bernoulli('y', theta[group_idx], observed = data_mix)\n    out_mixed = pm.sample(nuts_sampler='nutpie')\n\nsum_stats_mix = az.summary(out_mixed, kind = 'stats')\n\nsum_stats_mix\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.74\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.73\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.78\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.79\n                    7\n                \n            \n            \n        \n    \n\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.75\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.74\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.76\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.76\n                    3\n                \n            \n            \n        \n    \n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\nmu_logodds__\n-0.936\n0.569\n-1.991\n0.171\n\n\nnu_log__\n1.538\n0.696\n0.192\n2.754\n\n\ntheta_logodds__[0]\n0.212\n0.375\n-0.518\n0.896\n\n\ntheta_logodds__[1]\n-2.043\n0.550\n-3.084\n-1.027\n\n\ntheta_logodds__[2]\n-2.039\n0.575\n-3.157\n-1.027\n\n\nmu\n0.294\n0.112\n0.082\n0.492\n\n\nnu\n5.799\n3.849\n0.529\n12.569\n\n\ntheta[0]\n0.551\n0.090\n0.384\n0.721\n\n\ntheta[1]\n0.126\n0.056\n0.024\n0.226\n\n\ntheta[2]\n0.127\n0.060\n0.027\n0.235\n\n\n\n\n\n\n\nIf we look at the theta values for each value of the index in the mixed sample models we get some wonky results. Theta1 and Theta2 are the same because we made it that way. It is a bit hard to tell what is going on but if we plot the data we can see that theta0 is being brough closer to the global mean that it otherwise probably would have.\n\naz.plot_forest(data = out_mixed, var_names=['mu', 'theta'])\n\narray([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nWhy would we want this, shouldn‚Äôt the data speak for itself? In some respects yes and no. The primary benefit is that while we are maybe underestimating the effects of theta0 the shrinkage provides some robustness when modeling groups. While we may miss the target it will generally be shrunk transparently towards the global mean. This provides some of the same benefits as using something like a Student-T distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "mlm.html#a-soccer-example",
    "href": "mlm.html#a-soccer-example",
    "title": "3¬† Hierarchical Models",
    "section": "3.3 A Soccer example",
    "text": "3.3 A Soccer example\nThe modeling problem is that we are trying to model success rate. One of the key parts of the metric is the rate part. We are modeling productivity as a function of their shot attempts. We wouldn‚Äôt expect that a goalie is going to have the same success rate as Messi. But, critically they could have 100% success rate if they have 1 shot attempt and one goal. We could actually hand do the success rate ourselves. But, this may be less transparent as well as the fact that we are trying to be as lazy as possible.\nIn brms we are used to doing this as brms::brm(outcome | trials(1)) we are going to use a similar mechanism but instead of the formula syntax you are familiar with we are just passing it to a named argument. So the model is going to look like this\n$$\n\\[\\begin{aligned}\n\\text{Hyper Priors}\\\\\n\\mu \\sim Beta(1.7, 5,8) \\\\\n\\nu \\sim Gamma(125, 50) \\\\\n\\text{Intercept Priors} \\\\\n\\mu_{p} \\sim Beta(\\mu, \\nu ) \\\\\n\\sigma_{p} \\sim Gamma(125, 50) \\\\\n\\Theta \\sim Beta(\\mu_p, \\sigma_p) \\\\\nY \\sim Binomial(n_i,\\Theta)\n  \n\\end{aligned}\\]\n$$\nSo now when we model this it will look like\n\nsoccer = pl.read_csv('https://raw.githubusercontent.com/aloctavodia/BAP3/refs/heads/main/code/data/football_players.csv', schema_overrides={'position': pl.Categorical})\n\npos_idx = soccer['position'].to_physical().to_numpy()\npos_codes = soccer['position'].cat.get_categories().to_numpy()\n\nn_pos = len(pos_codes)\nn_players = soccer.height\n\ncoords = {'pos': pos_codes}\n\n\nwith pm.Model(coords = coords) as soccer_mod: \n    mu = pm.Beta('mu', 1.7, 5.8)\n    nu = pm.Gamma('nu', mu = 125, sigma = 50)\n    mu_p = pm.Beta('mu_p', mu = mu, nu = nu, dims = 'pos')\n    nu_p = pm.Gamma('nu_p', mu = 125, sigma = 50, dims = 'pos')\n    theta = pm.Beta('theta', mu = mu_p[pos_idx], nu = nu_p[pos_idx])\n    y = pm.Binomial('gs', n = soccer['shots'].to_numpy(), p = theta, observed = soccer['goals'].to_numpy())\n    out_soccer = pm.sample(draws=3000, target_accept=0.95, random_seed=4591, nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for a minute\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    4000\n                    0\n                    0.11\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    4000\n                    0\n                    0.11\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    4000\n                    0\n                    0.11\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    4000\n                    0\n                    0.11\n                    31\n                \n            \n            \n        \n    \n\n\n\nNow lets compare her to the book\n\n_, ax = plt.subplots(3, 1, figsize=(12, 6), sharex=True)\naz.plot_posterior(out_soccer, var_names='mu', ax=ax[0])\nax[0].set_title(r\"Global mean\")\naz.plot_posterior(out_soccer.posterior.sel(pos=\"FW\"), var_names='mu_p', ax=ax[1])\nax[1].set_title(r\"Forward position mean\")\naz.plot_posterior(out_soccer.posterior.sel(theta_dim_0=1457), var_names='theta', ax=ax[2])\nax[2].set_title(r\"Messi mean\")\n\nText(0.5, 1.0, 'Messi mean')",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "mlm.html#excericises",
    "href": "mlm.html#excericises",
    "title": "3¬† Hierarchical Models",
    "section": "3.4 Excericises",
    "text": "3.4 Excericises\n\nUsing your own words explain the difference between partial pooling, complete pooling, and no pooling.\n\nWe can really just think of them as how much information we are sharing across groups. In a no pooling model we are, in effect, modeling each individual unit with only its own information. So if we are modeling vote share for the U.S we are modeling vote share as a function of state level economic factors, state level demographics, and then state level unobserved time invariant factors. This is kind of your typical fixed effect model you would specify in Econ or Political Science. We are impliciltly making the assumption that CA and GA don‚Äôt really tell us that much about each other. Which is probably not an unfair assumption but we are throwing some information away that could make our modeling more robust. While somewhere like CA and GA probably don‚Äôt have a ton in common we would get some useful cross-sectional information from nearby southern states and/or county level information between Fulton and the San Francisco county.\nComplete pooling is really just the idea that we are going to toss everything into a jar. We are making the assumption that the individuality of each unit does not contribute much to our understanding. In general I don‚Äôt think in Political science we model things like this because well most of the time individual units have quirks. Tbh honest I don‚Äôt really thing any field models anything with complete pooling. The reason this is because we are kind of saying that these things are interchangeable. In reality nothing really there are not a lot of instances where we are modeling something where cross-sectional information tells us nothing.\nPartial pooling is a compromise between the two approaches. We are not saying that all these are interchangeable, but we are saying that something we learn about say the Baltimore Ravens can tell us something about other good teams. However, we are also acknowledging that there are certain things about the Baltimore Ravens that are unique to the team. Not every team has Lamar Jackson and Derrick Henry which present different running threats that opposing defenses has to respect. This changes what other teams‚Äô defense have to do because they still have deal with a Lamar scramble, while also accounting for his threat as a passer. This is different than say the Texans or the Lions where Goff and Stroud are less dynamic scramblers, but also play on good offenses.\n\nCreate a Hierarchical version of the tips model in Chapter 2\n\nThe original model looks like this\n$$\n\\[\\begin{aligned}\n\\mu \\sim Normal(0, 10)\\\\\n\\sigma \\sim HalfNormal(10, 4) \\\\\ny \\sim Normal(\\mu, \\sigma)\n\n\\end{aligned}\\]\n$$\nSo the hierarchical model looks something to this effect.\n$$\n\\[\\begin{aligned}\n\\mu_g \\sim Gamma(5, 2) \\\\\n\\sigma_g \\sim Gamma(2, 1.5) \\\\\n\\mu \\sim Normal(mu, sigma) \\\\\n\\sigma =  HalfNormal(1) \\\\\ny \\sim (mu_p, sigma_p)\n\n\\end{aligned}\\]\n$$\nOr something to this effect. One thing that actually became really clear to me in the excercise is that the hyperpriors that we are tuning for the group specific effects are going to be propigated through the global mean generally. Whereas the global uncertainty is something that we are going to have to tune ourselves.\n$$\n\\[\\begin{aligned}\n\\mu_{groups} \\sim Gamma(mu = 5 , sigma = 2) \\\\\n\\mu \\sim HalfNormal(\\sigma = \\mu{groups}) \\\\\n\\sigma \\sim HalfNormal(sigma = 1) \\\\\ny \\sim (\\mu, \\sigma)\n\n\\end{aligned}\\]\n$$\nSo the models will then just be\n\n\nCode\ntips = pl.read_csv('https://raw.githubusercontent.com/aloctavodia/BAP3/refs/heads/main/code/data/tips.csv')\n\ncategories = np.array([\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n\nday_enum = pl.Enum(categories)\n\n# Cast the 'day' column to Enum and get the physical codes\ntips = tips.with_columns(\n    pl.col(\"day\").cast(day_enum).alias(\"day_enum\")\n)\nidx = tips[\"day_enum\"].to_physical().to_numpy()\ntip = tips[\"tip\"]\n\n\ncoords = {'days': categories, 'days_flat': categories[idx]}\n\n\n\nwith pm.Model(coords = coords) as just_mean_hp:\n    mu_g = pm.Gamma('mu_g', mu = 5 , sigma = 2)\n    mu = pm.HalfNormal('mu', sigma = mu_g, dims = 'days')\n    sigma = pm.HalfNormal('sigma', sigma = 1, dims ='days')\n    y = pm.Gamma('y', mu = mu[idx], sigma = sigma[idx], observed = tip, dims = 'days_flat')\n    out_just_mean_hp = pm.sample(1000, nuts_sampler='nutpie')\n\nwith pm.Model(coords = coords) as all_hyper_priors: \n    mu_g = pm.Gamma('mu_group', 5, 2)\n    sigma_g = pm.Gamma('sigma_group',2, 1.5)\n    mu = pm.Normal('mu', mu_g, sigma_g, dims = 'days')\n    sigma = pm.HalfNormal('sigma', 1, dims = 'days')\n    y = pm.Gamma('y', mu = mu[idx], sigma = sigma[idx], observed=tip, dims = 'days_flat')\n    out_all_hyper_priors= pm.sample(1000, nuts_sampler='nutpie')\n\naz.plot_forest([out_just_mean_hp, out_all_hyper_priors], model_names=['Just Mu Tuned', 'Both Mu and Sigma Tuned'], var_names='mu', combined=True, r_hat=False, ess=False, figsize=(12, 3))\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.75\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.75\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.77\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.74\n                    7\n                \n            \n            \n        \n    \n\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    9\n                    0.55\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    9\n                    0.51\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    30\n                    0.51\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    19\n                    0.55\n                    7\n                \n            \n            \n        \n    \n\n\n\narray([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "Programming-Probabilistically.html#excercise",
    "href": "Programming-Probabilistically.html#excercise",
    "title": "2¬† Programming Probabilistically",
    "section": "2.6 Excercise",
    "text": "2.6 Excercise\n\nImplement the coal mining disasters model yourself\n\n\ndisaster_data = pl.Series(\n    [4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n    3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n    2, 2, 3, 4, 2, 1, 3, np.nan, 2, 1, 1, 1, 1, 3, 0, 0,\n    1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n    0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n    3, 3, 1, np.nan, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n    0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], strict = False\n    \n)\n\nyears = np.arange(1851, 1962)\n\nThe model is just a count model that looks like this\n\\[\n\\begin{split}\n\\begin{aligned}  \n  D_t &\\sim \\text{Pois}(r_t), r_t= \\begin{cases}\n   e, & \\text{if } t \\le s \\\\\n   l, & \\text{if } t \\gt s\n   \\end{cases} \\\\\n  s &\\sim \\text{Unif}(t_l, t_h)\\\\         \n  e &\\sim \\text{exp}(1)\\\\\n  l &\\sim \\text{exp}(1)    \n\\end{aligned}\n\\end{split}\n\\]\nWhere the model is specified as: - Dt: The number of disasters in year - rt: The rate parameter of the Poisson distribution of disasters in year t - s: The year in which the rate parameter changes (the switchpoint). - e: The rate parameter before the switchpoint s - l The rate parameter after the switchpoint s - tl, th: The lower and upper boundaries of year t\n\nwith pm.Model() as coal_model:\n    switchpoint = pm.DiscreteUniform('switchpoint', lower = years.min(), upper = years.max())\n    e = pm.Exponential('e', 1.0)\n    l = pm.Exponential('l', 1.0)\n    rate = pm.math.switch(switchpoint &gt;= years, e, l)\n    dt = pm.Poisson('dt', rate, observed = disaster_data)\n    coal_mod_out = pm.sample()\n\n/Users/josh/Library/CloudStorage/Dropbox/learning-bayes/bayesian-analysis-with-python/.venv/lib/python3.12/site-packages/pymc/model/core.py:1286: RuntimeWarning: invalid value encountered in cast\n  data = convert_observed_data(data).astype(rv_var.dtype)\n/Users/josh/Library/CloudStorage/Dropbox/learning-bayes/bayesian-analysis-with-python/.venv/lib/python3.12/site-packages/pymc/model/core.py:1300: ImputationWarning: Data in dt contains missing values and will be automatically imputed from the sampling distribution.\n  warnings.warn(impute_message, ImputationWarning)\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;CompoundStep\n&gt;&gt;Metropolis: [switchpoint]\n&gt;&gt;Metropolis: [dt_unobserved]\n&gt;NUTS: [e, l]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Programming Probabilistically</span>"
    ]
  }
]