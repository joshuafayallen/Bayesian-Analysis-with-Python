[
  {
    "objectID": "thinking-probabilistically.html",
    "href": "thinking-probabilistically.html",
    "title": "1  Thinking Probabilistically",
    "section": "",
    "text": "1.1 A Probability Primer for Bayesian Practitioners",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "thinking-probabilistically.html#a-probability-primer-for-bayesian-practitioners",
    "href": "thinking-probabilistically.html#a-probability-primer-for-bayesian-practitioners",
    "title": "1  Thinking Probabilistically",
    "section": "",
    "text": "1.1.1 Sample space and events\nLets say we go out and ask 3 random respondents a series of three questions and record their answers. There are lots of ways we could represent. In Table 1.1 I remix the example in the book a bit but the core idea is the same. The sample space is the set of all possible outcomes.\n\n\n\n\nTable 1.1\n\n\n\n\n\n\n\n\n\nQuestion\nIndividual\nResponse\n\n\n\n\n1\nPerson 1\nyes\n\n\n2\nPerson 1\nyes\n\n\n3\nPerson 1\nyes\n\n\n1\nPerson 2\nno\n\n\n2\nPerson 2\nno\n\n\n3\nPerson 2\nno\n\n\n\n\n\n\n\n\n\n\n\nIn our case, we are just talking about the order of each person’s responses. An event is a subset of the feature space, so we can think of it as the order in which people respond to the questions. So there are eight possible combinations of answer orders. For brevity, I only include two of those events. If we wanted to know how likely it is that we observe an event, we can express it as a simple proportion.\n\\[\nP(Event) = \\frac{\\text{event we are interested in}}{\\text{total number of events}}\n\\]\nSo if we are interested in the all yes event, it is just 0.125. If we make the assumption that the events are independent of each other then the probability of observing an event where the respondent answers all yes and another respondent answers all no than we can represent that as:\n\\[\nP(\\text{All Yes and No}) = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = 0.25\n\\]\nFast forward a bit the three axioms of probability are:\n\nThe probability of an event is an even is a non-negative real number\nThe probability of P(sample space) = 1\nIf 𝐴1, 𝐴2,… are disjoint events, meaning they cannot occur simultaneously then 𝑃(𝐴1, 𝐴2, …) = 𝑃(𝐴1) + 𝑃(𝐴2) + …\n\n\n\n1.1.2 Random Variables\nA random variable is effectively a function that maps the sample space the real numbers. So we can think of that as. Doing something akin to:\n\n\n\n\n\n\n\n\nQuestion\nIndividual\nResponse\nMapping Examp\n\n\n\n\n1\nPerson 1\nyes\n1\n\n\n2\nPerson 1\nyes\n1\n\n\n3\nPerson 1\nyes\n1\n\n\n1\nPerson 2\nno\n0\n\n\n2\nPerson 2\nno\n0\n\n\n3\nPerson 2\nno\n0\n\n\n\n\n\n\n\n\nThe author uses the example of a set of Python code. So if the take example from pl.when(thing_happens).then(do this).otherwise(do that) this is an abstraction over a set of Rust code that takes our inputs and then maps them to our desired output. We could theoretically write out all this Rust code every single time. But we package it in a nice set of functions to make the end user’s code cleaner and reusable we make some functions to do this.\nRandom variables play this similar role in stats where we have some inputs like responses to a survey or temperature that get mapped onto some process. There is no randomness when moving from “yes” to “one”, but there is some randomness when we go to the survey the third and fourth person to get their event.\n\n\n1.1.3 Discrete random variables and their distributions\nWe generally don’t care about the kidns of problems that get presented in these refreshers. The probability of 3 inidividuals answering all yes or the probability of getting a one on the dice. In general we are more intersted in getting a list of probabilities for all possible answers. We can then get the possibilities of any sort of event we would care about. As a simple heuristic the list of the probabilities are effectively just this list of probabilities. So if we wanted to get the proportion of heads of 1000 flipped coins we could do this.\n\n\n\n\n\n\n\n(array([  1.,   0.,   0.,   2.,   2.,   3.,  31., 888.,  72.,   1.]),\n array([0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n        0.33333333, 0.4       , 0.46666667, 0.53333333, 0.6       ,\n        0.66666667]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1.1\n\n\n\n\nIn this case the probability distribution of a coin flip is a Bernouli distribution. When we have multiple Bernouli trials we can stack them and make a binomial. One thing about these functions is that they have, for lack of a better term, shape parameters. For a normal those are our mean and standard deviation for something like the beta binomial in rbinom() it is rbinom(shape1 = 2, shape2 = 9) which changes where the most likely outcomes are.\n\n\nCode\nnp.random.seed(1994)\n\nfrom cycler import cycler\ndefault_cycler = cycler(color=[\"#000000\", \"#6a6a6a\", \"#bebebe\", \"#2a2eec\"])\n\nplt.rc('axes', prop_cycle=default_cycler)\nplt.rc('figure', dpi=600)\n\nn = 5\n\nx = np.arange(0,5)\nfig, axes = plt.subplots(2,2, figsize = (10,6), sharey= True, sharex = True)\naxes = np.ravel(axes)\n\n\nfor ax, (alpha, bt) in zip(axes, ((1,1), (6,4),(4,6), (20,20))):\n    dist_pmf = binom(n, x) * (beta(x+alpha, n-x+bt) / beta(alpha, bt))\n    ax.vlines(x, 0, dist_pmf, colors = 'C0', lw=4)\n    ax.set_title(f\"α={alpha}, β={bt}\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(x + 1)\n\nfig.text(0.52, -0.04, \"x\", fontsize=18)\nfig.text(-0.04, 0.4, \"P(X=x)\", fontsize=18, rotation=\"vertical\")\n\n\nText(-0.04, 0.4, 'P(X=x)')\n\n\n\n\n\n\n\n\n\nWhile this is helpful for us as we are toying around with setting priors or as pedagogy. The thing that is doing the heavy lifting of mapping things to the like P(thing) = 0.6 is the probability mass function. This works for discrete data because you can’t have 1.5 heads. I eschew including it but one really helpful thing to help us wrap our heads around what changing the shape parameters of our distribution are is using the interactive plots.\n\nbb = pz.BetaBinomial(alpha = 10, beta = 10, n = 6)\n\nbb.plot_interactive(figsize = (10,6), pointinterval=False, xy_lim =(20,20))\n\n\n\n\n\n\n1.1.4 Conditional Probablities\nFrom the highest level of abstraction all our models are really just fancy conditional probability machines. Given an outcome variable how does it change if we observe these 1 to a million things. To keep it more manageable than lets just consider the bivariate case. If we are trying to understand how something about how good an NFL team is we might want to look at their EPA and condition on adjusted games lost. The more games that a team’s best players lose the worse their EPA is going to be. By knowing something about EPA we know a little bit something about their\nWe would anticipate that the higher a team’s EPA is the less adjusted games lost they have. If we plot the marginal distributions we can see that. Quickly to compute the marginal distribution of AGL we are going to take the p(AGL, EPA) and then we average over EPA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Thinking Probabilistically</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]